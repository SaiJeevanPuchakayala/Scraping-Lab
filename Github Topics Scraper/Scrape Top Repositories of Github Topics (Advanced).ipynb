{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Top Repositories of Github Topics (Advanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Web Scraping?\n",
    "`Web scraping` is the process of extracting and parsing data from websites in an automated fashion using a computer program. \n",
    "Itâ€™s a useful technique for creating datasets for research and learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Outline:\n",
    "\n",
    "* I'm going to scrape https://github.com/topics\n",
    "* I'm get a list of topics. For each topic, I will get topic title, topic page url and topic description.\n",
    "\n",
    "* For each topic, I will get top 25 repositories in the topic from topic page.\n",
    "* For each repository, I will grab the repo name, username, stars and repo URL.\n",
    "* For each topic, I will create a csv file in the following format shown below:\n",
    "\n",
    "`\n",
    "Repo Name,Username,Stars,Repo URL\n",
    "three.js,mrdoob,69700,https://github.com/mrdoob/three.js\n",
    "libgdx,libgdx,18300,https://github.com/libgdx/libgdx\n",
    "`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing a single function to:\n",
    "- Get the list of topics from the topics page\n",
    "- Get the list of top repos from the individual topic pages\n",
    "- For each topic, create a CSV of the top repos for the topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL to scrape\n",
    "topics_url = 'https://github.com/topics'\n",
    "response = requests.get(topics_url)\n",
    "page_contents = response.text\n",
    "doc = BeautifulSoup(page_contents, 'html.parser')\n",
    "base_url = \"https://github.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting Star count to numerical value\n",
    "def parse_star_count(stars_str):\n",
    "    stars_str = stars_str.strip()\n",
    "    if stars_str[-1] == 'k':\n",
    "        return int(float(stars_str[:-1])*1000)\n",
    "    return int(stars_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to downlaod the topic page\n",
    "def get_topic_page(topic_url):\n",
    "    response = requests.get(topic_url)\n",
    "    # Checking successful response\n",
    "    if response.status_code != 200:\n",
    "        raise Exception('Failed to load page {}'.format(topic_url))\n",
    "    # Parse using Beautiful soup\n",
    "    topic_doc = BeautifulSoup(response.text, 'html.parser')\n",
    "    return topic_doc\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Function to extract information about the repo\n",
    "def get_repo_info(h1_tag, star_tags):\n",
    "    # returns all the required info about a repository\n",
    "    a_tags = h1_tag.find_all('a')\n",
    "    username = a_tags[0].text.strip()\n",
    "    repo_name = a_tags[1].text.strip()\n",
    "    repo_url =  base_url + a_tags[1]['href']\n",
    "    stars = parse_star_count(star_tags.text.strip())\n",
    "    return username, repo_name, repo_url, stars\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Function to extract repos of particular topic\n",
    "def get_topic_repos(topic_doc):\n",
    "    # Get the h1 tags containing repo title, repo URL and username\n",
    "    h1_selection_class = 'f3 color-text-secondary text-normal lh-condensed'\n",
    "    repo_tags = topic_doc.find_all('h1', {'class': h1_selection_class} )\n",
    "    # Get star tags\n",
    "    star_tags = topic_doc.find_all('a', { 'class': 'social-count float-none'})\n",
    "    \n",
    "    topic_repos_dict = { 'UserName': [], 'RepoName': [],'RepoURL': [],'StarsCount': []}\n",
    "\n",
    "    # Get repo info\n",
    "    for i in range(len(repo_tags)):\n",
    "        repo_info = get_repo_info(repo_tags[i], star_tags[i])\n",
    "        topic_repos_dict['UserName'].append(repo_info[0])\n",
    "        topic_repos_dict['RepoName'].append(repo_info[1])\n",
    "        topic_repos_dict['RepoURL'].append(repo_info[2])\n",
    "        topic_repos_dict['StarsCount'].append(repo_info[3])\n",
    "        \n",
    "    return pd.DataFrame(topic_repos_dict)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Function to create CSV files which contains repo info for each topic\n",
    "def scrape_topic(topic_url, path):\n",
    "    if os.path.exists(path):\n",
    "        print(\"The file {} already exists. Skipping...\\n\".format(path))\n",
    "        return\n",
    "    topic_df = get_topic_repos(get_topic_page(topic_url))\n",
    "    topic_df.to_csv(path, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funtion to extract topic titles\n",
    "def get_topic_titles(doc):\n",
    "    selection_class = 'f3 lh-condensed mb-0 mt-1 Link--primary'\n",
    "    topic_title_tags = doc.find_all('p', {'class': selection_class})\n",
    "    topic_titles = []\n",
    "    for tag in topic_title_tags:\n",
    "        topic_titles.append(tag.text)\n",
    "    return topic_titles\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Function to extract topic description\n",
    "def get_topic_descs(doc):\n",
    "    desc_selector = 'f5 color-text-secondary mb-0 mt-1'\n",
    "    topic_desc_tags = doc.find_all('p', {'class': desc_selector})\n",
    "    topic_descs = []\n",
    "    for tag in topic_desc_tags:\n",
    "        topic_descs.append(tag.text.strip())\n",
    "    return topic_descs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Funtion to extract topic URL's\n",
    "def get_topic_urls(doc):\n",
    "    topic_link_tags = doc.find_all('a', {'class': 'd-flex no-underline'})\n",
    "    topic_urls = []\n",
    "    base_url = 'https://github.com'\n",
    "    for tag in topic_link_tags:\n",
    "        topic_urls.append(base_url + tag['href'])\n",
    "    return topic_urls\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "# Function to create Topics DataFrame\n",
    "def scrape_topics():\n",
    "    topics_url = 'https://github.com/topics'\n",
    "    response = requests.get(topics_url)\n",
    "    if response.status_code != 200:\n",
    "        print('Failed to load page {}'.format(topic_url))\n",
    "    topics_dict = {\n",
    "        'title': get_topic_titles(doc),\n",
    "        'description': get_topic_descs(doc),\n",
    "        'url': get_topic_urls(doc)\n",
    "    }\n",
    "    return pd.DataFrame(topics_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below is the single function which scrapes and extracts info of 30 repositories for each top topic on github, creates a dataframe for each topic, convert the dataframe to CSV file, and Save it in the \n",
    "`GithubTopicsData` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_topics_repos():\n",
    "    print('Scraping list of topics')\n",
    "    topics_df = scrape_topics()\n",
    "    \n",
    "    os.makedirs('GithubTopicsData', exist_ok=True)\n",
    "    for index, row in topics_df.iterrows():\n",
    "        print('Scraping top repositories of topic: \"{}\"'.format(row['title']))\n",
    "        r = requests.get(row['url'])\n",
    "        if r.status_code != 200:\n",
    "            print('Failed to load page {}\\n'.format(row['url']))\n",
    "        else:\n",
    "            scrape_topic(row['url'], 'GithubTopicsData/{}.csv'.format(row['title']))\n",
    "    return \"=================================== Done ====================================\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping list of topics\n",
      "Scraping top repositories of topic: \"3D\"\n",
      "The file GithubTopicsData/3D.csv already exists. Skipping...\n",
      "\n",
      "Scraping top repositories of topic: \"Ajax\"\n",
      "The file GithubTopicsData/Ajax.csv already exists. Skipping...\n",
      "\n",
      "Scraping top repositories of topic: \"Algorithm\"\n",
      "The file GithubTopicsData/Algorithm.csv already exists. Skipping...\n",
      "\n",
      "Scraping top repositories of topic: \"Amp\"\n",
      "The file GithubTopicsData/Amp.csv already exists. Skipping...\n",
      "\n",
      "Scraping top repositories of topic: \"Android\"\n",
      "The file GithubTopicsData/Android.csv already exists. Skipping...\n",
      "\n",
      "Scraping top repositories of topic: \"Angular\"\n",
      "The file GithubTopicsData/Angular.csv already exists. Skipping...\n",
      "\n",
      "Scraping top repositories of topic: \"Ansible\"\n",
      "The file GithubTopicsData/Ansible.csv already exists. Skipping...\n",
      "\n",
      "Scraping top repositories of topic: \"API\"\n",
      "The file GithubTopicsData/API.csv already exists. Skipping...\n",
      "\n",
      "Scraping top repositories of topic: \"Arduino\"\n",
      "The file GithubTopicsData/Arduino.csv already exists. Skipping...\n",
      "\n",
      "Scraping top repositories of topic: \"ASP.NET\"\n",
      "The file GithubTopicsData/ASP.NET.csv already exists. Skipping...\n",
      "\n",
      "Scraping top repositories of topic: \"Atom\"\n",
      "The file GithubTopicsData/Atom.csv already exists. Skipping...\n",
      "\n",
      "Scraping top repositories of topic: \"Awesome Lists\"\n",
      "The file GithubTopicsData/Awesome Lists.csv already exists. Skipping...\n",
      "\n",
      "Scraping top repositories of topic: \"Amazon Web Services\"\n",
      "The file GithubTopicsData/Amazon Web Services.csv already exists. Skipping...\n",
      "\n",
      "Scraping top repositories of topic: \"Azure\"\n",
      "The file GithubTopicsData/Azure.csv already exists. Skipping...\n",
      "\n",
      "Scraping top repositories of topic: \"Babel\"\n",
      "The file GithubTopicsData/Babel.csv already exists. Skipping...\n",
      "\n",
      "Scraping top repositories of topic: \"Bash\"\n",
      "The file GithubTopicsData/Bash.csv already exists. Skipping...\n",
      "\n",
      "Scraping top repositories of topic: \"Bitcoin\"\n",
      "The file GithubTopicsData/Bitcoin.csv already exists. Skipping...\n",
      "\n",
      "Scraping top repositories of topic: \"Bootstrap\"\n",
      "The file GithubTopicsData/Bootstrap.csv already exists. Skipping...\n",
      "\n",
      "Scraping top repositories of topic: \"Bot\"\n",
      "The file GithubTopicsData/Bot.csv already exists. Skipping...\n",
      "\n",
      "Scraping top repositories of topic: \"C\"\n",
      "The file GithubTopicsData/C.csv already exists. Skipping...\n",
      "\n",
      "Scraping top repositories of topic: \"Chrome\"\n",
      "The file GithubTopicsData/Chrome.csv already exists. Skipping...\n",
      "\n",
      "Scraping top repositories of topic: \"Chrome extension\"\n",
      "The file GithubTopicsData/Chrome extension.csv already exists. Skipping...\n",
      "\n",
      "Scraping top repositories of topic: \"Command line interface\"\n",
      "Scraping top repositories of topic: \"Clojure\"\n",
      "Failed to load page https://github.com/topics/clojure\n",
      "\n",
      "Scraping top repositories of topic: \"Code quality\"\n",
      "Failed to load page https://github.com/topics/code-quality\n",
      "\n",
      "Scraping top repositories of topic: \"Code review\"\n",
      "Failed to load page https://github.com/topics/code-review\n",
      "\n",
      "Scraping top repositories of topic: \"Compiler\"\n",
      "Failed to load page https://github.com/topics/compiler\n",
      "\n",
      "Scraping top repositories of topic: \"Continuous integration\"\n",
      "Failed to load page https://github.com/topics/continuous-integration\n",
      "\n",
      "Scraping top repositories of topic: \"COVID-19\"\n",
      "Failed to load page https://github.com/topics/covid-19\n",
      "\n",
      "Scraping top repositories of topic: \"C++\"\n",
      "Failed to load page https://github.com/topics/cpp\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'=================================== Done ===================================='"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scrape_topics_repos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def scrape_topics_repos():\n",
    "    print('Scraping list of topics')\n",
    "    topics_df = scrape_topics()\n",
    "    \n",
    "    os.makedirs('GithubTopicsData', exist_ok=True)\n",
    "    for index, row in topics_df.iterrows():\n",
    "        print('Scraping top repositories of topic: \"{}\"'.format(row['title']))\n",
    "        r = requests.get(row['url'])\n",
    "        if r.status_code != 200:\n",
    "            time.sleep(5)\n",
    "        scrape_topic(row['url'], 'GithubTopicsData/{}.csv'.format(row['title']))\n",
    "    return \"=================================== Done ====================================\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping list of topics\n",
      "Scraping top repositories of topic: \"3D\"\n",
      "The file GithubTopicsData/3D.csv already exists. Skipping...\n",
      "\n",
      "Scraping top repositories of topic: \"Ajax\"\n",
      "The file GithubTopicsData/Ajax.csv already exists. Skipping...\n",
      "\n",
      "Scraping top repositories of topic: \"Algorithm\"\n",
      "The file GithubTopicsData/Algorithm.csv already exists. Skipping...\n",
      "\n",
      "Scraping top repositories of topic: \"Amp\"\n",
      "The file GithubTopicsData/Amp.csv already exists. Skipping...\n",
      "\n",
      "Scraping top repositories of topic: \"Android\"\n",
      "The file GithubTopicsData/Android.csv already exists. Skipping...\n",
      "\n",
      "Scraping top repositories of topic: \"Angular\"\n",
      "The file GithubTopicsData/Angular.csv already exists. Skipping...\n",
      "\n",
      "Scraping top repositories of topic: \"Ansible\"\n",
      "The file GithubTopicsData/Ansible.csv already exists. Skipping...\n",
      "\n",
      "Scraping top repositories of topic: \"API\"\n",
      "The file GithubTopicsData/API.csv already exists. Skipping...\n",
      "\n",
      "Scraping top repositories of topic: \"Arduino\"\n",
      "The file GithubTopicsData/Arduino.csv already exists. Skipping...\n",
      "\n",
      "Scraping top repositories of topic: \"ASP.NET\"\n",
      "The file GithubTopicsData/ASP.NET.csv already exists. Skipping...\n",
      "\n",
      "Scraping top repositories of topic: \"Atom\"\n",
      "The file GithubTopicsData/Atom.csv already exists. Skipping...\n",
      "\n",
      "Scraping top repositories of topic: \"Awesome Lists\"\n",
      "The file GithubTopicsData/Awesome Lists.csv already exists. Skipping...\n",
      "\n",
      "Scraping top repositories of topic: \"Amazon Web Services\"\n",
      "The file GithubTopicsData/Amazon Web Services.csv already exists. Skipping...\n",
      "\n",
      "Scraping top repositories of topic: \"Azure\"\n",
      "The file GithubTopicsData/Azure.csv already exists. Skipping...\n",
      "\n",
      "Scraping top repositories of topic: \"Babel\"\n",
      "The file GithubTopicsData/Babel.csv already exists. Skipping...\n",
      "\n",
      "Scraping top repositories of topic: \"Bash\"\n",
      "The file GithubTopicsData/Bash.csv already exists. Skipping...\n",
      "\n",
      "Scraping top repositories of topic: \"Bitcoin\"\n",
      "The file GithubTopicsData/Bitcoin.csv already exists. Skipping...\n",
      "\n",
      "Scraping top repositories of topic: \"Bootstrap\"\n",
      "The file GithubTopicsData/Bootstrap.csv already exists. Skipping...\n",
      "\n",
      "Scraping top repositories of topic: \"Bot\"\n",
      "The file GithubTopicsData/Bot.csv already exists. Skipping...\n",
      "\n",
      "Scraping top repositories of topic: \"C\"\n",
      "The file GithubTopicsData/C.csv already exists. Skipping...\n",
      "\n",
      "Scraping top repositories of topic: \"Chrome\"\n",
      "The file GithubTopicsData/Chrome.csv already exists. Skipping...\n",
      "\n",
      "Scraping top repositories of topic: \"Chrome extension\"\n",
      "The file GithubTopicsData/Chrome extension.csv already exists. Skipping...\n",
      "\n",
      "Scraping top repositories of topic: \"Command line interface\"\n",
      "The file GithubTopicsData/Command line interface.csv already exists. Skipping...\n",
      "\n",
      "Scraping top repositories of topic: \"Clojure\"\n",
      "Scraping top repositories of topic: \"Code quality\"\n",
      "Scraping top repositories of topic: \"Code review\"\n",
      "Scraping top repositories of topic: \"Compiler\"\n",
      "Scraping top repositories of topic: \"Continuous integration\"\n",
      "Scraping top repositories of topic: \"COVID-19\"\n",
      "Scraping top repositories of topic: \"C++\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'=================================== Done ===================================='"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scrapetime_topics_repos() # Single function with time delay to match internet speeds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
